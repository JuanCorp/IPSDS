{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://mlxprodcontent.blob.core.windows.net/014929-1000/en-us/thumbnail.png?v=20161129182519)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Note: This notebook will have almost no code, however I don't suggest that you skip it.*\n",
    "\n",
    "In #TheRealWorld, most enterprise data is stored in what's called a database. As its name suggests, it's a place where the data that is created from several applcations is stored and later retrieved for whatever uses the place has for it. Databases are incredibly useful since they store the data indepently of the application that is actually creating the data, so other applications can connect to it and also read the data. In more technical terms, the database is on a separate server, so applications just need the IP or the connection string to connect to it, and start working with it.\n",
    "\n",
    "Data in databases is stored in tables, where each row represents a record, and each column represents a feature or information. It's very similar to the dataframes we've been working with so far. \n",
    "\n",
    "![title](http://www.plus2net.com/sql_tutorial/images/table.jpg)\n",
    "\n",
    "\n",
    "In this example, EmployeeID, Name, Age, Salary and Department give some information about a particular entity. We can assume that this table or entity is  **Employees** because of the information stored in it. Each row or record represents an individual employee, which their respective information for each column. The employee ID or number would represent the unique index, as we saw in pandas, for the employees entity. In a database table this is known as a **primary key**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine that each department had more information than just a name. However we already have the department name included on the Employees table, and it would be inefficient to look for a department by name, in a company with 100 Departments. Reading our employees for a bit, we find out that each employee works for one department, but multiple employees can work for one department. This means that there is a **one to many** relationship between departments and employees, since one department has one or more employees. \n",
    "\n",
    "![title](https://www.xml.com/pub/a/2003/03/05/graphics/emp-dep.gif)\n",
    "\n",
    "(Not the same table but you get me). In this case, a department has its id, the id of the manager of the department, and the department name. If you look closely, you can see that the Employees table is passing on the employeeID of the manager of the table to the departments table, and the departments table is passing on its primary key, the department id, to the employees table. Now, each employee has a designated departmentID which points to a specific department, and each department has a managerID that points to a specific employee. These IDs are known as **Foreign Keys** and allow to tables to be more easily related to one another, by specifying a key from another table. Since primary and foreign keys are, for the most part, indexes, accessing them is just as fast as we would access a row index from a list or a dataframe. We just use the index key, and boom, we get the associated row. This is the basis for **Relational Databases** where each entity is associated to another entity, forming a structured and organized way of storing and accesing data.\n",
    "\n",
    "![title](http://www.w3resource.com/postgresql-exercises/database-model.gif) \n",
    "*A sample HR relational database schema. Notice how each table is connected to another table.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, imagine we already had our database set up with some data stored in it. Programmatically, how would we access it? In databases,  the language for querying data is called **SQL** or Structured Query Language. Unlike Python, where the interpreter tries to make sense of whatever you wrote, in SQL you directly command what you want the engine to do for you. \n",
    "\n",
    "In our earlier example, to get all first names and last names from all employees, a query would look something like this:\n",
    "\n",
    "    SELECT first_name,last_name\n",
    "    FROM employees\n",
    "\n",
    "SELECT indicates the columns to return from the table, while  FROM indicates which table to look in. Just like we've done in pandas, we can specify  some conditions on which data to return.\n",
    "\n",
    "    SELECT first_name,last_name\n",
    "    FROM employees\n",
    "    WHERE salary > 60000\n",
    "\n",
    "This query will return all employee first names and last names, for employees whose salary is  greater than 60000.  \n",
    "\n",
    "You can also group data using GROUP BY, order data using ORDER BY, and what defines relational databases the most, JOINS. JOINS merge two tables into one temporarily based on a common key. \n",
    "\n",
    "    SELECT last_name, job_title \n",
    "    FROM employees E JOIN jobs J on (E.job_id = J.job_id)  \n",
    "\n",
    "This will return the last names and job titles for all employees, something that would not be possible with only one table individually since both columns reside on different tables, but are joined together by their keys.  \n",
    "\n",
    "There's so much to talk about concerning SQL that I can't possibly cover it here. Some SQL databases include:\n",
    "\n",
    "- Oracle SQL Database, SQL Developer.\n",
    "\n",
    "- Microsoft SQL Server, T-SQL. \n",
    "\n",
    "- Oracle MySQL.\n",
    "\n",
    "- PostGRE SQL\n",
    "\n",
    "To learn more about SQL: https://www.w3schools.com/sql/default.asp\n",
    "\n",
    "To learn more about how to use SQL with Python, check this: http://www.python-course.eu/sql_python.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine that instead of having an application that manages the database, we now have 1000 data sources. These sources come from the usual applications, social media, websites, and others. This could be 1,10,100 GBs per minute or more, with various data types like: tables, transactions, text, images and sound in real time. When you have are facing a situation like this, you are dealing with **Big Data**. There's a lot of definitions of what Big Data is, or what it is about, but my take on the matter is that you are dealing with Big Data, when you are dealing with a huge amount of varied data in a short amount of time. When you reach that point, traditional ways of handling data aren't as efficient when operating.\n",
    "\n",
    "![title](http://semeon.com/blog/wp-content/uploads/2017/02/big-data.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes up Big Data are the 3 Vs: Volume, Velocity, Variety.\n",
    "\n",
    "- **Volume**: Refers to the amount of data that is being processed. In Big Data scenarios, this can get to TBs,PBs of data or more!\n",
    "\n",
    "- **Velocity**: Refers to the speed in which the data is being processed. In Big Data scenarios, the goal is real-time data processing.\n",
    "\n",
    "- **Variety**: Refers to the types of data being processed. These include images, video, audio, text, and structured data.\n",
    "\n",
    "![title](https://www.mssqltips.com/tipimages2/3132_BigData_ThreeVsOfBigData.jpg) \n",
    "\n",
    "Some say it's 4 Vs, including the Veracity of the data, or how truthful or clean the data is. Some include 5 or 6 Vs, but most stick to the 3 or 4 mentioned earlier. Now this all seems interesting, but how do we deal with this in real problems? The way to dealing with this is using the MapReduce programming model. \n",
    "\n",
    "First we **Map** the data by sorting or filtering or slicing the data, then we **Reduce** the data by applying a summary operation like calculating the counts, frequencies, mean and so on. Finally, we join the split data back together. All this is done by distributing the data through multiple servers or machines, which run these tasks in parallel, and finally join their results together. This allows the operations to be scalable, thus allowing the possibility for a big data arquitecture.\n",
    "\n",
    "Some tools that help with this are:\n",
    "\n",
    "- Hadoop: https://www.tutorialspoint.com/hadoop/index.htm\n",
    "\n",
    "- Spark: https://www.tutorialspoint.com/apache_spark/apache_spark_introduction.htm Lightining speed, in-memory data processing.\n",
    "\n",
    "- Mrjob: https://pythonhosted.org/mrjob/ Python-based library for writing Hadoop jobs on Python.\n",
    "\n",
    "- Luigi: https://pypi.python.org/pypi/luigi Framework that helps at building and automating pipelines of multiple jobs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
