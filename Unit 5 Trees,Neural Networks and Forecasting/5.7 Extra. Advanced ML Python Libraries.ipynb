{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://image.slidesharecdn.com/meetmltalkbyjaroslawszymczakaboutxgboost-160428092528/95/xgboost-the-algorithm-that-wins-every-competition-9-638.jpg?cb=1461835587)\n",
    "\n",
    "<sub>Source: https://www.slideshare.net/JaroslawSzymczak1/xgboost-the-algorithm-that-wins-every-competition </sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll see some of the implementations of advanced algorithms used widely today. First off, two Gradient Boosting implementations:\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "XGBoost or eXtreme Gradient Boosting, is an implementation of the Gradient Boosted Trees algorithm that we saw earlier but with two twists:\n",
    "\n",
    "- It's made to be **scalable, portable and accurate**, including support for parallelization and the use of Graphical Processing Units (GPUs). The performance on large datasets is nothing to snuff at, and it's used on production machine learning systems, and used as a winner of many kaggle competitions, either as a standalone model or a stacked model.Stacking is similar to the ensemble we saw previously, but instead of using a lot of weak predictors, we use add the results of a few strong predictors  as features to a model. More info: http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/. \n",
    "\n",
    "- It implements **regularization**. Just like we saw with LASSO and ElasticNet, XGBoost adds regularization during boosting to create a less complex model that doesn't overfit as much. \n",
    "\n",
    "There's way more to it than that however, in the little things. \n",
    "\n",
    "For more information on it: http://xgboost.readthedocs.io/en/latest/\n",
    "\n",
    "And most importantly, how to install it (in Windows, it's a nightmare): https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en\n",
    "\n",
    "It also comes with an sklearn like API, which allows for the usual fit,predict, score, etc. Aside from that, here are some of the few key differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0345808e10a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m \u001b[1;31m#When it's installed, this should work.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb #When it's installed, this should work.\n",
    "import numpy as np\n",
    "\n",
    "data = np.random.randn(100)\n",
    "labels = np.sin(data)\n",
    "\n",
    "dtrain = xgb.DMatrix( data, label=label) #Need to create special xgboost dataset object for training and testing.\n",
    "\n",
    "params = {'eta':1, 'silent':1, 'objective':'binary:logistic' } #learning rate is eta instead\n",
    "\n",
    "\n",
    "xgb_model = xgb.train( params, dtrain, num_boost_rounds = 100 ) #Instead of fit.\n",
    "\n",
    "\n",
    "test = xgb.DMatrix(np.random.randn(10))\n",
    "xgb_model.predict(test) #This is still the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM\n",
    "\n",
    "Light Gradient Boosting Machine or LGBM for short, is a framework for gradient boosted trees that implements leaf-wise growth instead of depth-wise growth. What this means is that the splits will be focused on one split first, until it reaches it's end (is all leaves), then performs gradient descent, and finally, splits the other side of the tree. As a visual example: \n",
    "\n",
    "![title](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/11194110/leaf.png)\n",
    "Regular GBM\n",
    "\n",
    "![title](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/11194227/depth.png)\n",
    "\n",
    "Light GBM\n",
    "\n",
    "<sub> Source: https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/ </sub>\n",
    "\n",
    "\n",
    "This type of splitting ends up being faster, at the cost of more overfitting. However with the addition of regularization, and a maximum depth, we get a fast and robust gradient boosting model, that uses low memory consumption, and works efficiently even with large amounts of data. Like XGBoost, it has its own differences in syntax:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb #When it's installed, this should work.\n",
    "\n",
    "data = np.random.randn(100)\n",
    "labels = np.cos(data)\n",
    "\n",
    "dtrain = lgb.Dataset( data, label=label) #Need to create special xgboost dataset object for training and testing.\n",
    "\n",
    "params = {'eta':1, 'silent':1, 'objective':'binary:logistic' } #learning rate is eta instead\n",
    "\n",
    "\n",
    "lgb_model = lgb.train( params, dtrain, num_boost_rounds = 100 ) #Instead of fit.\n",
    "\n",
    "\n",
    "test = np.random.randn(10)\n",
    "lgb_model.predict(test) #This is still the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Neural Networks, we'll some later on unit 6, but some of these include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TensorFlow: Low level deep neural networks: https://www.tensorflow.org/ \n",
    "\n",
    "- Keras: High level deep neural networks: https://keras.io/\n",
    "\n",
    "(Low level means high complexity but with high  ability for customization, while high level is the opposite.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
