{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](http://cs231n.github.io/assets/nn2/dropout.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after learning how to create and train a neural network on the last notebook, we successfully classified images between digits that are 0 or 1. In this notebook we'll go further beyond that, but we'll focus on two things: building a more general neural network class and applying some extra generalization and optimization functions for the network . We'll also use the full dataset for classifying handwritten digits, so the complexity is now greater! Let's first start with all the extra tidbits. \n",
    "\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Now before you go dropping this notebook, this is not about dropping the course. Dropout in neural networks is a process applied after the activation function where a proportion of results of the activation function are multiplied element-wise by a vector of the same size. This vector is the same size as the output of the activation function, and is filled with 1s and 0s. So after this multiplication, some of the results of the activation function are turned to 0 and others stay the same, essentially \"dropping\" some of the results. For example, if the dropout ratio was 0.5 then we would get:\n",
    "\n",
    "- $y = f(x) \\cdot m$ \n",
    "\n",
    "where $y$ is the output of the neuron, $f(x)$ is the activation function and m is the dropout vector, which would be 50% (0.5 dropout ratio) filled with 0s and the rest filled with 1s. A fully connected (Linear) layer occupies most of the parameters and neurons develop co-dependency amongst each other during training which curbs the individual power of each neuron leading to over-fitting of training data. With dropout, this dependency is reduced by a large amount, since each training epoch generates a \"new\" neural network. During testing, the weights are simply multiplied by the dropout ratio, \"simulating\" the dropout while using all neurons.\n",
    "\n",
    "- $z = p \\cdot Wy$ where p is the dropout ratio.\n",
    "\n",
    "During backpropagation, we multiply the derivative of the activation function by the same dropout vector $m$, thus simulating the same dropout experienced in forward propagation during backpropagation.\n",
    "\n",
    "## Adding Noise\n",
    "\n",
    "Since the point of ML models is to be able to generalize to unknown testing data, perturbing the inputs by a small bit can make the network more robust to unknown inputs. However, it's bad to just add random data that may even hinder our performance, although with images this isn't that huge of a problem as long as the noise is small. This noise is typically added to the input layer, before any calculations are made, where random noise is added to all the pixels of the image. This makes the network put more effort in training and generalizing across all inputs. \n",
    "\n",
    "\n",
    "## Decaying learning rate \n",
    "\n",
    "If we use a validation set, and validate the network on each training epoch with the test data, we can get an overall score of how the model is improving. If we find that the model isn't improving after a certain number of epochs, we can make the learning rate smaller so that the model changes its weights at a slower rate, but gets closer to the minimum loss. We may choose to divide the learning rate by 2 or another number after a fixed number of epochs, or we wait until our validation accuracy isn't improving, and cut the learning rate there. If it doesn't improve after two decays, we can also implement **early stopping** where we stop the training of the neural network, since it's already done training. This speeds up the training procedure considerably.\n",
    "\n",
    "## Optimizations\n",
    "\n",
    "Aside from Stochastic Gradient Descent, we can implement other variations of gradient descent based on SGD, like:\n",
    "\n",
    "### Momentum\n",
    "Momentum  is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction $\\gamma$ of the update vector of the past time step to the current update vector:\n",
    "\n",
    "$v_t = \\gamma v_{t-1} + \\alpha \\nabla_\\theta J(\\theta)  $ where $\\alpha$ is the learning rate, $\\nabla_\\theta J(\\theta)$ is the gradient of the parameters, and $\\gamma$ is the momentum fraction, usually started at 0.9\n",
    "\n",
    "### Nesterov Momentum\n",
    "\n",
    "This is a variation of momentum in which we subtract the velocity parameter $\\gamma v_{t-1}$ to the weights before calculating the gradient. This gives us a rough estimate of where the parameters are going ot be, and makes training more efficient.\n",
    "\n",
    "$v_t = \\gamma v_{t-1} + \\alpha \\nabla_\\theta J(\\theta - \\gamma v_{t-1})$\n",
    "\n",
    "\n",
    "### Adagrad\n",
    "\n",
    "This is an optimization algorithm which uses an adaptive learning rate to update the parameters, by performing larger updates for infrequent parameters and smaller updates for frequent parameters. This is done by saving a history of the parameter updating or a \"cache\".\n",
    "\n",
    "$\\theta_{t+1} = \\theta_t  - \\dfrac{\\alpha}{\\sqrt{G_t + \\varepsilon}} * g_t$ where $G_t$ is the accumulation of the parameter update history, g_t is the gradient of the parameter and $\\varepsilon$ is an extremely small number like $ 10^{-8}$ \n",
    "\n",
    "### Adam\n",
    "\n",
    "This is a method keeps an exponentially decaying average of past gradients $m_t$ like momentum, while also keeping  an exponentially decaying average of past squared gradients $v_t$ (different $v$ than the one in momentum), where:\n",
    "\n",
    "$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t$\n",
    "\n",
    "$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g{_t}^2$ \n",
    "\n",
    "but these are biased towards 0, so to counteract them they are reprashed as:\n",
    "\n",
    "$\\hat{m_t} = \\dfrac{m_t}{1 - \\beta_{1}^t}$\n",
    "\n",
    "$\\hat{v_t} = \\dfrac{v_t}{1 - \\beta_{2}^t}$\n",
    "\n",
    "where $\\beta_1$ is initialized at 0.9 and $\\beta_2$ is initialized at 0.999. The final parameter updated is phrased:\n",
    "\n",
    "$\\theta_t = \\theta_t - \\dfrac{\\alpha}{\\sqrt{\\hat{v_t}} + \\varepsilon} \\hat{m_t}$\n",
    "\n",
    "And mamy more optimizations, but we will leave them until this.\n",
    "\n",
    "\n",
    "\n",
    "## More Generalized NN\n",
    "\n",
    "Finally, let's build our more generalized neural network class. It should accept:\n",
    "\n",
    "- An indefinite amount of layers. This is done by having a list of layers and adding layers one by one. Then the forward and backward procedure is executed through each of them.\n",
    "\n",
    "- Dividing Linear and activation layers.\n",
    "\n",
    "- Activation Layers must accept different activation functions and dropout. \n",
    "\n",
    "- Input layer can accept noise.\n",
    "\n",
    "- Linear Layers must accept different optimization functions.\n",
    "\n",
    "So let's get to business. We'll first create the general network class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Network(object):\n",
    "    \n",
    "    def __init__(self,data,labels,layers = [],epochs = 10, batch_size = 100,decay = False):\n",
    "        #Initialize general network parameters.\n",
    "        self.Xtrain,self.Xtest,self.ytrain,self.ytest = train_test_split(data,labels,test_size = 0.2)\n",
    "        self.y = labels\n",
    "        self.layers = layers\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.decay = decay\n",
    "        self.train_size = len(self.Xtrain)\n",
    "        self.test_errors = []\n",
    "        \n",
    "        \n",
    "    def Train(self,noise = False):\n",
    "        for e in range(self.epochs):\n",
    "            perm = np.random.permutation(len(self.Xtrain))\n",
    "            self.Xtrain = self.Xtrain[perm]\n",
    "            self.ytrain = self.ytrain[perm]\n",
    "            \n",
    "            for i in range(self.train_size // self.batch_size):\n",
    "                x_batch = self.Xtrain[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                y_batch = self.ytrain[i * self.batch_size : (i + 1) * self.batch_size]\n",
    "                if noise == True:\n",
    "                    x_batch += np.random.normal(0,1,x_batch.shape) / 255 #Add random noise to input layer.\n",
    "                \n",
    "                forward = self.layers[0].forward(x_batch,train = True)\n",
    "                #Forward\n",
    "                for l in range(1,len(self.layers) - 1):\n",
    "                    forward = self.layers[l].forward(forward,train = True)\n",
    "                \n",
    "                #Final prediction\n",
    "                final_forward = self.layers[-1].forward(forward,y_batch)\n",
    "                \n",
    "                #Backprop\n",
    "                backward = self.layers[-1].backward() #Cross entropy layer.\n",
    "                for l in range(len(self.layers)-2,0,-1):\n",
    "                    backward = self.layers[l].backward(backward,self.decay)\n",
    "                self.decay = False #Update in every iteration so it only decays once after being changed.\n",
    "            \n",
    "            self.Test(e)     \n",
    "        \n",
    "    def Test(self,epoch):\n",
    "        #Test the network against the validation data.\n",
    "        train_forward = self.layers[0].forward(self.Xtrain,train = False)\n",
    "        for l in range(1,len(self.layers)-1):\n",
    "            train_forward = self.layers[l].forward(train_forward,train = False)\n",
    "        \n",
    "        train_forward = self.layers[-1].forward(train_forward,self.ytrain)\n",
    "        \n",
    "        test_forward = self.layers[0].forward(self.Xtest,train = False)\n",
    "        for l in range(1,len(self.layers) - 1):\n",
    "            test_forward = self.layers[l].forward(test_forward,train = False)\n",
    "            \n",
    "        test_forward = self.layers[-1].forward(test_forward,self.ytest)\n",
    "        \n",
    "        train_error = self.Loss(train_forward,self.ytrain)\n",
    "        test_error = self.Loss(test_forward,self.ytest)\n",
    "        self.test_errors.append(test_error)\n",
    "        if self.test_errors[epoch] > self.test_errors[epoch - 1]:\n",
    "            self.decay = True\n",
    "            \n",
    "        print('\\rEnd of epoch, train error :  ', train_error, '  Test error : ', test_error)\n",
    "        \n",
    "        \n",
    "    def Loss(self,prediction,ground_truth):\n",
    "        #Calculate the accuracy of the predictions.\n",
    "        prediction_labels = np.argmax(prediction,axis = 1)\n",
    "        true_labels = np.argmax(ground_truth,axis = 1)\n",
    "        accuracy = np.sum(np.equal(prediction_labels,true_labels))/ float(np.shape(true_labels)[0])\n",
    "        return 1 - accuracy\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the key parts, the layers. We'll create base class that all other layers will inherit from, since they have things in common, like their input and output and their forward and backwards method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create Linear Layers. This one's easy, since we've seen how they work in the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    \n",
    "    def __init__(self,Weights,bias,optimizer = None):\n",
    "        super().__init__()\n",
    "        self.Weights = (np.random.standard_normal(Weights) / 100).astype(np.float64)\n",
    "        self.bias = (np.random.randn(bias) / 100).astype(np.float64)\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def forward(self,X,train = True):\n",
    "        prediction =  np.add(np.dot(X,self.Weights), self.bias) #Weight Multiplication and adding bias.\n",
    "        \n",
    "        if train:\n",
    "            self.input = X\n",
    "            self.output = prediction\n",
    "            \n",
    "        return prediction\n",
    "        \n",
    "    def backward(self,previous_error,decay):\n",
    "        dX = np.dot(previous_error, self.Weights.T)\n",
    "        self.update_parameters(previous_error,decay)\n",
    "        return dX\n",
    "    \n",
    "    def update_parameters(self,error,decay):\n",
    "        dW = np.dot(self.input.T, error)\n",
    "        db = np.sum(error, axis = 0)\n",
    "        W_update,b_update = self.optimizer.optimize(dW,db,decay)\n",
    "        self.Weights -= W_update / self.input.shape[0]\n",
    "        self.bias -= b_update / self.input.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's proceed to create the activation layer. This layer will accept as a parameter the activation function it's calculating and if it's doing dropout or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    \n",
    "    def __init__(self,activation,dropout = None):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self,X,train = True):\n",
    "        prediction = self.activation.function(X)\n",
    "        if train:\n",
    "            self.input = X\n",
    "            if self.dropout:\n",
    "                self.m = self.drop(prediction,self.dropout)\n",
    "                prediction = np.multiply(prediction,self.m)\n",
    "                \n",
    "            self.output = prediction\n",
    "        else:\n",
    "            if self.dropout:\n",
    "                prediction = prediction * self.dropout\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def backward(self,previous_error,decay = None):\n",
    "        dX = self.activation.derivative(previous_error)\n",
    "        if self.dropout:\n",
    "            dX = np.multiply(dX,self.m)\n",
    "        return dX\n",
    "    \n",
    "    def drop(self,X,dropout):\n",
    "        return np.random.binomial(1,dropout,size = X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also create the cross entropy derivative class, which we'll use for the output layer. Again, this was defined in the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Cross_Entropy(Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self,prediction,truth):\n",
    "        self.input = truth\n",
    "        self.output = self.softmax(prediction)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self):\n",
    "        return self.output - self.input\n",
    "    \n",
    "    def softmax(self,z):\n",
    "        e = np.exp(z)\n",
    "        s = e.sum(axis=1)\n",
    "        s_reshape = np.repeat(s, z.shape[1]).reshape(z.shape)\n",
    "        return e/s_reshape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not done! As you can see we haven't defined the activation functions themselves, or the optimizations. Let's start with the activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Activation(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.x  = None\n",
    "        self.dx = None\n",
    "        \n",
    "    def function(self):\n",
    "        pass\n",
    "    \n",
    "    def derivative(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define at least three activation functions. Let's start with sigmoid, which we already know by heart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def function(self,z):\n",
    "        denominator = (1 + np.exp(-z))\n",
    "        self.x = 1/denominator\n",
    "        return self.x\n",
    "    \n",
    "    def derivative(self,z = None):\n",
    "        self.dx = self.x * (1 - self.x)\n",
    "        return np.multiply(z,self.dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go with another familiar one, tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TanH(Activation):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def function(self,z):\n",
    "        denominator =  1 + np.exp(-2 * z)\n",
    "        self.x = (2 / denominator) - 1\n",
    "        return self.x\n",
    "    \n",
    "    def derivative(self,z):\n",
    "        self.dx = 1 - self.x ** 2\n",
    "        return np.multiply(z,self.dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a last one we'll use the rectified linear unit or ReLU. This function only returns positive values or 0. It's pretty simple to implement.\n",
    "\n",
    "![title](https://wikimedia.org/api/rest_v1/media/math/render/svg/8d1e78eaf8445e3c1a9d48229abb921a61f30bad)\n",
    "\n",
    "And the derivative:\n",
    "\n",
    "![title](https://wikimedia.org/api/rest_v1/media/math/render/svg/29ee90b67c01654d3efba98c6fd13d21f75855f1)\n",
    "\n",
    "And the graph looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGwBChuAGgJAhuAEgZP4fM/c739nIdDIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa222940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reluX = np.arange(-5,5,0.1)\n",
    "relu = lambda x: np.where(x >= 0,x,0)\n",
    "y = relu(reluX)\n",
    "plt.plot(reluX,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data = None\n",
    "        \n",
    "    def function(self,z):\n",
    "        self.data = z\n",
    "        self.x = np.where(z >= 0.,z,0.).astype(np.float64)\n",
    "        return self.x\n",
    "    \n",
    "    def derivative(self,z):\n",
    "        self.dx = np.where(self.data >= 0., 1. ,0.).astype(np.float64)\n",
    "        return np.multiply(z,self.dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more activation functions here: https://en.wikipedia.org/wiki/Activation_function \n",
    "\n",
    "Let's now proceed the optimizations we described earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    \n",
    "    def __init__(self,learning_rate = 0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        pass\n",
    "    \n",
    "    def optimize(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SGD \n",
    "\n",
    "$\\theta = \\theta - \\nabla_\\theta J(\\theta)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self,learning_rate = 0.1):\n",
    "        super().__init__(learning_rate)\n",
    "        \n",
    "    def optimize(self,dW,db,decay):\n",
    "        if decay:\n",
    "            self.learning_rate /= 2\n",
    "            \n",
    "        return dW * self.learning_rate,db * self.learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Momentum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$v_t = \\gamma v_{t-1} + \\alpha \\nabla_\\theta J(\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Momentum(Optimizer):\n",
    "    \n",
    "    def __init__(self,Weight_size,bias_size,momentum_ratio = 0.9,learning_rate = 0.01):\n",
    "        super().__init__(learning_rate)\n",
    "        self.weight_updates = np.zeros(Weight_size)\n",
    "        self.bias_updates = np.zeros(bias_size)\n",
    "        self.ratio = momentum_ratio\n",
    "        \n",
    "    \n",
    "    def optimize(self,dW,db,decay):\n",
    "        if decay:\n",
    "            self.learning_rate /= 2\n",
    "        self.weight_updates = (self.ratio * self.weight_updates) + (self.learning_rate * dW)\n",
    "        self.bias_updates = (self.ratio * self.bias_updates) + (self.learning_rate * db)\n",
    "        return self.weight_updates,self.bias_updates\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adagrad\n",
    "\n",
    "$\\theta_{t+1} = \\theta_t  - \\dfrac{\\alpha}{\\sqrt{G_t + \\varepsilon}} * g_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adagrad(Optimizer):\n",
    "    def __init__(self,Weight_size,bias_size,learning_rate = 1,epsilon = 1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.weight_updates = np.zeros(Weight_size)\n",
    "        self.bias_updates = np.zeros(bias_size)\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    \n",
    "    def optimize(self,dW,db,decay):\n",
    "        if decay:\n",
    "            self.learning_rate /= 2\n",
    "            \n",
    "        self.weight_updates += dW ** 2\n",
    "        self.bias_updates += db ** 2\n",
    "        new_weight = (self.learning_rate / np.sqrt(self.weight_updates + self.epsilon)) * dW\n",
    "        new_bias = (self.learning_rate / np.sqrt(self.bias_updates + self.epsilon)) * db\n",
    "        return new_weight,new_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adam\n",
    "\n",
    "$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1)g_t$\n",
    "\n",
    "$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2)g{_t}^2$ \n",
    "\n",
    "--\n",
    "\n",
    "$\\hat{m_t} = \\dfrac{m_t}{1 - \\beta_{1}^t}$\n",
    "\n",
    "$\\hat{v_t} = \\dfrac{v_t}{1 - \\beta_{2}^t}$\n",
    "\n",
    "--\n",
    "\n",
    "$\\theta_t = \\theta_t - \\dfrac{\\alpha}{\\sqrt{\\hat{v_t}} + \\varepsilon} \\hat{m_t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self,Weight_size,bias_size,learning_rate = 0.1,beta1 = 0.9,beta2= 0.999,epsilon = 1e-8):\n",
    "        super().__init__(learning_rate)\n",
    "        self.weight_momentum = np.zeros(Weight_size)\n",
    "        self.weight_velocity = np.zeros(Weight_size)\n",
    "        self.bias_momentum = np.zeros(bias_size)\n",
    "        self.bias_velocity = np.zeros(bias_size)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    \n",
    "    def optimize(self,dW,db,decay):\n",
    "        if decay:\n",
    "            self.learning_rate /= 2\n",
    "        \n",
    "        self.weight_momentum = (self.beta1 * self.weight_momentum) + (1 - self.beta1) * dW\n",
    "        self.weight_velocity = (self.beta2 * self.weight_velocity) + (1 - self.beta2) * (dW ** 2)\n",
    "        \n",
    "        self.bias_momentum = (self.beta1 * self.bias_momentum) + (1 - self.beta1) * db\n",
    "        self.bias_velocity = (self.beta2 * self.bias_velocity) + (1 - self.beta2) * (db ** 2)\n",
    "        \n",
    "        mw_hat = self.weight_momentum / (1 - self.beta1)\n",
    "        vw_hat = self.weight_velocity / (1 - self.beta2)\n",
    "        \n",
    "        mb_hat = self.bias_momentum / (1 - self.beta1)\n",
    "        vb_hat = self.bias_velocity / (1 - self.beta2)\n",
    "        \n",
    "        new_weight = (self.learning_rate /(np.sqrt(vw_hat) + self.epsilon)) * mw_hat\n",
    "        new_bias = (self.learning_rate /(np.sqrt(vb_hat) + self.epsilon)) * mb_hat\n",
    "        return new_weight,new_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay with that we are done adding classes. That sure was long and maybe confusing, but that just goes to show you how complex building a robust neural network is. Let's now test it.\n",
    "\n",
    "## Testing the Network.\n",
    "\n",
    "First let's load all the images like last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "digits = pd.read_csv(\"digits.csv\")\n",
    "digits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the values of the pixels of the image are in numbers between 0 and 255, and since that scale may affect the performance of some of our algorithms, we'll rescale the pixels to values between 0 and 1, by dividing all pixels by 255. When displaying the image, we just have to multiply the pixels by 255, and they'll be back to normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = digits.drop(\"label\",axis = 1).values.astype(np.float64) / 255\n",
    "labels = pd.get_dummies(digits[\"label\"]).values.astype(np.float64) #One hot\n",
    "del digits # Save memory!\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate the network first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = Network(images,labels,epochs = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now all the layers. Let's first recreate the network we built in the previous notebook on this notebook, but with more outputs since we are predicting more than two labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#1st. \n",
    "first = LinearLayer(Weights = (784,256),bias = 256,optimizer = Adam((784,256),256))\n",
    "network.layers.append(first)\n",
    "\n",
    "#2nd\n",
    "second = ActivationLayer(activation = TanH())\n",
    "network.layers.append(second)\n",
    "\n",
    "#3rd \n",
    "third = LinearLayer(Weights = (256,100),bias = 100, optimizer = Adam((256,100),100))\n",
    "network.layers.append(third)\n",
    "\n",
    "#4th\n",
    "\n",
    "fourth= ActivationLayer(activation = TanH())\n",
    "network.layers.append(fourth)\n",
    "\n",
    "fifth = LinearLayer(Weights = (100,10),bias = 10, optimizer = Adam((100,10),10))\n",
    "network.layers.append(fifth)\n",
    "\n",
    "sixth= ActivationLayer(activation = TanH())\n",
    "network.layers.append(sixth)\n",
    "\n",
    "output = Cross_Entropy()\n",
    "network.layers.append(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that's missing is training the network, and seeing how it does..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch, train error :   0.14494047619   Test error :  0.149642857143\n",
      "End of epoch, train error :   0.121220238095   Test error :  0.124047619048\n",
      "End of epoch, train error :   0.111845238095   Test error :  0.115\n",
      "End of epoch, train error :   0.108035714286   Test error :  0.11\n",
      "End of epoch, train error :   0.104255952381   Test error :  0.107619047619\n",
      "End of epoch, train error :   0.101488095238   Test error :  0.104642857143\n",
      "End of epoch, train error :   0.0995238095238   Test error :  0.101666666667\n",
      "End of epoch, train error :   0.096130952381   Test error :  0.100357142857\n",
      "End of epoch, train error :   0.0937202380952   Test error :  0.0988095238095\n",
      "End of epoch, train error :   0.0919047619048   Test error :  0.0983333333333\n",
      "End of epoch, train error :   0.0899107142857   Test error :  0.0960714285714\n",
      "End of epoch, train error :   0.0885714285714   Test error :  0.0953571428571\n",
      "End of epoch, train error :   0.0873214285714   Test error :  0.0944047619048\n",
      "End of epoch, train error :   0.0859821428571   Test error :  0.0932142857143\n",
      "End of epoch, train error :   0.0846726190476   Test error :  0.092619047619\n",
      "End of epoch, train error :   0.0834226190476   Test error :  0.0909523809524\n",
      "End of epoch, train error :   0.0821130952381   Test error :  0.089880952381\n",
      "End of epoch, train error :   0.0811011904762   Test error :  0.0884523809524\n",
      "End of epoch, train error :   0.0802083333333   Test error :  0.087619047619\n",
      "End of epoch, train error :   0.0783333333333   Test error :  0.0855952380952\n"
     ]
    }
   ],
   "source": [
    "network.Train(noise = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So our previous network would get a 90% accuracy on the test set if used on the full dataset. Let's try with a more complex network now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "New_Layers = []\n",
    "#1st. \n",
    "first2 = LinearLayer(Weights = (784,392),bias = 392, optimizer = SGD())\n",
    "New_Layers.append(first2)\n",
    "\n",
    "#2nd\n",
    "second2 = ActivationLayer(activation = ReLU())\n",
    "New_Layers.append(second2)\n",
    "\n",
    "#3rd \n",
    "third2 = LinearLayer(Weights = (392,196),bias = 196, optimizer = Adam((392,196),196))\n",
    "New_Layers.append(third2)\n",
    "\n",
    "#4th\n",
    "fourth2 = ActivationLayer(activation = TanH(), dropout = 0.5)\n",
    "New_Layers.append(fourth2)\n",
    "\n",
    "#5th\n",
    "fifth2 = LinearLayer(Weights = (196,98),bias = 98, optimizer = Adagrad((196,98),98))\n",
    "New_Layers.append(fifth2)\n",
    "\n",
    "#6th\n",
    "sixth2 = ActivationLayer(activation = Sigmoid())\n",
    "New_Layers.append(sixth2)\n",
    "\n",
    "#7th\n",
    "seventh2 = LinearLayer(Weights = (98,10),bias = 10, optimizer = Momentum((98,10),10)) \n",
    "New_Layers.append(seventh2)\n",
    "\n",
    "output2 = Cross_Entropy()\n",
    "New_Layers.append(output2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch, train error :   0.22869047619   Test error :  0.232142857143\n",
      "End of epoch, train error :   0.168660714286   Test error :  0.170476190476\n",
      "End of epoch, train error :   0.14244047619   Test error :  0.145119047619\n",
      "End of epoch, train error :   0.127202380952   Test error :  0.130952380952\n",
      "End of epoch, train error :   0.116369047619   Test error :  0.117976190476\n",
      "End of epoch, train error :   0.109047619048   Test error :  0.1125\n",
      "End of epoch, train error :   0.0991964285714   Test error :  0.104523809524\n",
      "End of epoch, train error :   0.0954166666667   Test error :  0.0997619047619\n",
      "End of epoch, train error :   0.0896130952381   Test error :  0.0947619047619\n",
      "End of epoch, train error :   0.086755952381   Test error :  0.0913095238095\n",
      "End of epoch, train error :   0.0846428571429   Test error :  0.0911904761905\n",
      "End of epoch, train error :   0.0816964285714   Test error :  0.0870238095238\n",
      "End of epoch, train error :   0.0791071428571   Test error :  0.0845238095238\n",
      "End of epoch, train error :   0.0778869047619   Test error :  0.0836904761905\n",
      "End of epoch, train error :   0.0749702380952   Test error :  0.0821428571429\n",
      "End of epoch, train error :   0.0738095238095   Test error :  0.0817857142857\n",
      "End of epoch, train error :   0.0716666666667   Test error :  0.0791666666667\n",
      "End of epoch, train error :   0.070625   Test error :  0.0804761904762\n",
      "End of epoch, train error :   0.0694345238095   Test error :  0.0767857142857\n",
      "End of epoch, train error :   0.0687202380952   Test error :  0.0769047619048\n",
      "End of epoch, train error :   0.0683333333333   Test error :  0.0757142857143\n",
      "End of epoch, train error :   0.0673511904762   Test error :  0.0758333333333\n",
      "End of epoch, train error :   0.0672619047619   Test error :  0.0763095238095\n",
      "End of epoch, train error :   0.0675595238095   Test error :  0.0763095238095\n",
      "End of epoch, train error :   0.0670833333333   Test error :  0.0763095238095\n",
      "End of epoch, train error :   0.0673214285714   Test error :  0.075\n",
      "End of epoch, train error :   0.0672619047619   Test error :  0.0759523809524\n",
      "End of epoch, train error :   0.067380952381   Test error :  0.0758333333333\n",
      "End of epoch, train error :   0.0677083333333   Test error :  0.0757142857143\n",
      "End of epoch, train error :   0.0675   Test error :  0.0757142857143\n"
     ]
    }
   ],
   "source": [
    "network2 = Network(images,labels,epochs = 30, layers = New_Layers)\n",
    "network2.Train(noise = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this network arquitecture gets a better accuracy when testing than our preivous one. Experiment with various different architectures, and find out which ones work and which ones dont!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "NN Optimization: http://cs231n.github.io/optimization-1/\n",
    "NN Architecture: https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
